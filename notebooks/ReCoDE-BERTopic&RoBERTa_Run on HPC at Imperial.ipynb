{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on HPC at Imperial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Tensorflow environment\n",
    "Follow the instructions in \"conda\" section this link https://icl-rcs-user-guide.readthedocs.io/en/latest/hpc/applications/guides/tensorflow/ to setup Tensorflow environment, enabling to utilize GPU on HPC\n",
    "\n",
    "### Install requried packages\n",
    "After setting up the environment, execute the following steps to install required packages for supporting the code to run.\n",
    "  -  module load anaconda3/personal\n",
    "  -  source activate \"your virtual env name from the setup\"\n",
    "  -  python3 -m pip install \"tensorflow-text==2.15.*\"\n",
    "  -  python3 -m pip install \"tf-models-official==2.15.*\"\n",
    "  -  python3 -m pip install bertopic\n",
    "  -  python3 -m pip install xlrd\n",
    "  -  python3 -m pip install umap-learn hdbscan\n",
    "  -  python3 -m pip install nbformat\n",
    "\n",
    "### Prepare data\n",
    "Upload the data to HPC. The data (5 .xls files) are downloadable from this GitHub repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 09:43:32.302386: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-29 09:43:32.302522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-29 09:43:32.493936: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-29 09:43:33.041933: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-29 09:44:09.948603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/rds/general/user/jgao/home/anaconda3/envs/tf3_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil # Import the shutil module for file operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Set TensorFlow logger level to ERROR to suppress unnecessary output\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file created successfully.\n"
     ]
    }
   ],
   "source": [
    "# List of file names\n",
    "file_names = [\n",
    "    \"Web_of_Science_Search_1-1000 results.xls\",\n",
    "    \"Web_of_Science_Search_1001-2000 results.xls\",\n",
    "    \"Web_of_Science_Search_2001-3000 results.xls\",\n",
    "    \"Web_of_Science_Search_3001-4000 results.xls\",\n",
    "    \"Web_of_Science_Search_4001-5000 results.xls\"\n",
    "]\n",
    "\n",
    "# List to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Read each Excel file and select the desired columns\n",
    "for file_name in file_names:\n",
    "    df = pd.read_excel(file_name)\n",
    "    df_selected = df[[\"Publication Type\", \"Authors\", \"Article Title\", \"Source Title\", \"Abstract\", \"Publication Year\", \"DOI\"]]\n",
    "    dfs.append(df_selected)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Write the merged dataframe to a CSV file\n",
    "merged_df.to_csv(\"Web_of_Science_Query May 07 2024_1-5000.csv\", index=False)\n",
    "\n",
    "print(\"Merged CSV file created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Publication Type                                            Authors  \\\n",
      "0                J                             Campbell, S; Gurney, L   \n",
      "1                J                            Carstens, M; Preiser, R   \n",
      "2                J  Manuel-Navarrete, D; DeLuca, S; Friso, F; Poli...   \n",
      "3                J                    Carmen, E; Fazey, I; Friend, RM   \n",
      "4                J  Griesberger, P; Kunz, F; Hacklaender, K; Matts...   \n",
      "\n",
      "                                       Article Title            Source Title  \\\n",
      "0  What are we protecting? Rethinking relational ...   ECOSYSTEMS AND PEOPLE   \n",
      "1  Exploring relationality in African knowledge s...   ECOSYSTEMS AND PEOPLE   \n",
      "2  Ayahuasca ceremonies, relationality, and inner...   ECOSYSTEMS AND PEOPLE   \n",
      "3  Community-based sustainability initiatives: th...  SUSTAINABILITY SCIENCE   \n",
      "4  Building a decision-support tool to inform sus...                   AMBIO   \n",
      "\n",
      "                                            Abstract  Publication Year  \\\n",
      "0  Relational values have been proposed as a way ...              2024   \n",
      "1  The current solutions offered by Western susta...              2024   \n",
      "2  The use of psychedelic substances is increasin...              2024   \n",
      "3  The quality of relationships between different...              2024   \n",
      "4  In wildlife management, differing perspectives...              2024   \n",
      "\n",
      "                             DOI  \n",
      "0  10.1080/26395916.2024.2315973  \n",
      "1  10.1080/26395916.2024.2315995  \n",
      "2  10.1080/26395916.2024.2339227  \n",
      "3     10.1007/s11625-024-01492-8  \n",
      "4     10.1007/s13280-024-02020-9  \n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"Web_of_Science_Query May 07 2024_1-5000.csv\", encoding='utf-8')\n",
    "\n",
    "# Preview the data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 09:47:31,686 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 157/157 [00:11<00:00, 13.62it/s]\n",
      "2024-05-29 09:47:58,892 - BERTopic - Embedding - Completed ✓\n",
      "2024-05-29 09:47:58,893 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-05-29 09:48:35,139 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-05-29 09:48:35,142 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-05-29 09:48:35,294 - BERTopic - Cluster - Completed ✓\n",
      "2024-05-29 09:48:35,298 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-05-29 09:48:35,907 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data to handle null values\n",
    "df['Abstract'] = df['Abstract'].fillna('')  # Replace null values with empty strings\n",
    "\n",
    "# Create a BERTopic instance\n",
    "topic_model = BERTopic(verbose=True)\n",
    "\n",
    "# Fit the model on your dataset\n",
    "docs = df['Abstract'].tolist()\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 5000\n"
     ]
    }
   ],
   "source": [
    "num_documents = len(docs)\n",
    "print(\"Number of documents:\", num_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 157/157 [00:06<00:00, 25.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a SentenceTransformer model with the 'all-MiniLM-L6-v2' variant for generating embeddings\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize a BERTopic model with the specified SentenceTransformer embedding model and enable verbose mode for logging\n",
    "topic_model = BERTopic(embedding_model=embedding_model, verbose=True)\n",
    "\n",
    "# Encode the list of documents into embeddings using the initialized SentenceTransformer model,\n",
    "# showing a progress bar during the encoding process\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Save the embeddings to a NumPy array file (.npy)\n",
    "import numpy as np\n",
    "np.save('embeddings.npy', embeddings)  # Save to .npy file\n",
    "\n",
    "# Save the embeddings to a pickle file for serialization (.pkl)\n",
    "# Serialization refers to the process of converting an object into a format that can be easily stored, transmitted, or reconstructed later. In Python, serialization is commonly used for saving objects to files or transferring them between different systems.\n",
    "# The .pkl extension here denotes a pickle file, which is a binary file format used for serializing and deserializing objects. Pickle files can store various Python objects, such as lists, dictionaries, and even custom classes, in a compact and efficient binary format.\n",
    "import pickle\n",
    "with open('embeddings.pkl', 'wb') as file:\n",
    "    pickle.dump(embeddings, file)\n",
    "\n",
    "# Convert the embeddings into a pandas DataFrame for further analysis and export it to a CSV file without indexing\n",
    "import pandas as pd\n",
    "embeddings_df = pd.DataFrame(embeddings)\n",
    "embeddings_df.to_csv('embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 09:48:54,721 - BERTopic - Embedding - Transforming documents to embeddings.\n",
      "Batches: 100%|██████████| 157/157 [00:05<00:00, 27.72it/s]\n",
      "2024-05-29 09:49:00,624 - BERTopic - Embedding - Completed ✓\n",
      "2024-05-29 09:49:00,625 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-05-29 09:49:12,756 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-05-29 09:49:12,757 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2024-05-29 09:49:12,918 - BERTopic - Cluster - Completed ✓\n",
      "2024-05-29 09:49:12,921 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-05-29 09:49:13,441 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x14625d953610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics().write_html('topics_before_tuning.html', auto_open=True) # open the file in a separate tab in your web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=15).write_html('topics_barchart_before_tuning.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(top_n_topics=100).write_html('topics_hierachy_before_tuning.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(top_n_topics=100).write_html('topics_heatmap.before_tuning.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Relational values have been proposed as a way of capturing more inclusively the relationships that people have with nature and have been adopted within the conceptual framework of the Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services (IPBES). Relational values literature has taken strides towards a more comprehensive appreciation of human-nature interactions than previous frameworks. However, we see an opportunity to build further on the relational values concept through the frame of political ontology. In this Perspective, we argue that, in order to understand people's relationships with their environments, we must first ask the following question: what is nature to those who value their relationships with it? Comprehending the multiple natures that people experience and value can help us to achieve equitable and representative conservation policy, explain actions and behaviours, and identify obstacles to engagement with conservation agendas.\", 'The current solutions offered by Western sustainability science to address prevailing global environmental destruction and social injustice are still largely embedded in the Western knowledge system established by colonisation, limiting the efficacy of these solutions for a large part of the planet. Conversely, it may be reasonable to imagine that the concept of relationality is beneficial in all cultures and knowledge systems. Relationality, elementally referring to a web of relationships, as considered from an African Indigenous and local knowledge perspective, could play an important role in decolonising Western sustainability science. Two valuable approaches, namely ubuntu (humanness) and ukama (relatedness), as predominantly observed in southern Africa, are essentially immersed in human-nature relationality. This type of relationality considers everything as interconnected, and therefore that nothing happens in isolation, and that the well-being of humans is inextricable from the well-being of nature. The way relationality is approached in African Indigenous knowledge systems is inclusive, holistic and perpetual, broadening its usefulness to a large audience, making it a sensible contributor to decoloniality in sustainability science. A collective knowledge could emerge, including cooperative, multidirectional interactions with different types of information from diverse human and non-human sources, increasingly eradicating the relational divide among knowledge systems caused by persistent colonial discourse and attitudes.', \"The use of psychedelic substances is increasingly associated with nature-relatedness. We explore whether entheogenic uses of ayahuasca in settings co-produced between Indigenous and Western knowledges may also foster relationality and sustainability transformations across ontology, praxis, and epistemology. A survey with 74 English-speaking individuals who attended Amazonian healing ceremonies at the Takiwasi Center in Peru, along with 11 semi-structured interviews and a discussion circle revealed unexpected personal shifts towards relationality. Beyond the expected increase in nature-relatedness, participants also reported boundary dissolution and changes in their perceptions of self, leading them to experience nature and non-human beings as having spiritual or human-like agency. The blurring of perceived boundaries between themselves and nature also challenged the materialist ontologies in which they had been educated and socialized. In terms of both epistemologies and praxis, co-produced ayahuasca ceremonies enhanced relational thinking and embodiment of relationality. Inner-outer transformations ensued from the post-ceremonial integration of the 'plant's teachings' into participants' daily lives. We discuss our findings' contributions to the emerging field of inner transformations and the relational turn in sustainability. Potential sustainability benefits of scaling plant-based ceremonies need to be measured against their impacts on the Amazon rainforest and its biocultures. Relational Thinking: Ceremonial use of ayahuasca, when combining Indigenous and Western knowledge, fosters a shift in thinking that emphasizes connections between people and ecosystems, challenges rigid beliefs, and privileges more intuitive, relational ways of being in the world. This shift supports inner-outer transformations towards sustainability.Inner Transformations Leading to Sustainable Actions: Ayahuasca ceremonies were linked to shifts in lifestyle choices, decreased consumption of environmentally harmful substances, and a stronger inclination towards nature and community living.Indigenous Relationality: Ayahuasca ceremonies co-produced with Indigenous knowledge holders offer insights into reciprocity with non-humans, balanced living, and nurturing relationships, all of which are fundamental for sustainable futures.Ethical Considerations: Responsible and respectful usage of ayahuasca includes considering the sustainability of the plant, the ecosystems it grows in, and the Indigenous cultures sharing their knowledge. Supportive practices and trained guidance should accompany ayahuasca use, ensuring safety and ethical engagement.Knowledge co-production: Sustainability science can inform the design and implementation of co-produced ceremonial spaces and practices to ensure the sustainable inner-outer transformations reported in this study.\", 'The quality of relationships between different actors involved in community-based sustainability initiatives is central to their success. This study examines the role of the qualities of social relationships within 22 different community-based sustainability initiatives each framed round different types of sustainability challenges, from flooding and climate change to community development and youth engagement. Research involved 37 semi structured interviews, combined with visual techniques, to explore the qualities of different relationship from the perspective of actors actively engaged in developing and progressing initiatives with different communities across Scotland. A typology of relationship qualities (tense, pragmatic and supportive qualities) is presented and applied to examine the ways in which relationships shape the benefits that participants identify for these community-based initiatives. The findings show supportive relationship qualities, involving a sense of respect, integrity, honesty and opportunities to test out new ideas, are particularly important in sustainability initiatives, providing a diverse range of benefits. Relationship qualities can also shift over time, either relatively suddenly or incrementally. Some groups of initiative actors worked strategically with relationships, underpinning their relationship-based strategies with relationships with different actors dominated by supportive qualities to actively harness the benefits these types of relationships provide and strengthen the sense of community and shared interest surrounding initiatives. A focus on relationship qualities can therefore provide a more dynamic picture of how community-based initiatives unfold and adapt to increasingly complex challenges. Such an approach places human agency centre-stage, recognising the fundamental importance of shaping social relationships within community-based initiatives.', 'In wildlife management, differing perspectives among stakeholders generate conflicts about how to achieve disparate sustainability goals that include ecological, economic, and sociocultural dimensions. To mitigate such conflicts, decisions regarding wildlife management must be taken thoughtfully. To our knowledge, there exists no integrative modeling framework to inform these decisions, considering all dimensions of sustainability. We constructed a decision-support tool based on stakeholder workshops and a Bayesian decision network to inform management of wild ruminants in the federal state of Lower Austria. We use collaborative decision analysis to compare resource allocations while accounting for trade-offs among dimensions of sustainability. The tool is designed for application by non-technical users across diverse decision-making contexts with particular sets of wildlife management actions, objectives, and uncertainties. Our tool represents an important step toward developing and evaluating a transparent and replicable approach for mitigating wildlife-based conflicts in Europe and beyond.']\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('Web_of_Science_Query May 07 2024_1-5000.csv', encoding='utf-8')\n",
    "abstracts = df['Abstract'].dropna().tolist()  # Ensure no NaN values\n",
    "\n",
    "# Ensure all elements are strings\n",
    "abstracts = [str(abstract) for abstract in abstracts]\n",
    "\n",
    "# Debug: Print the first few elements to check\n",
    "print(abstracts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize in batches\n",
    "def batch_tokenize(texts, batch_size=32):\n",
    "    all_inputs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        all_inputs.append(inputs)\n",
    "    return all_inputs\n",
    "\n",
    "# Tokenize abstracts in batches\n",
    "batched_inputs = batch_tokenize(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings for each batch\n",
    "def batch_embed(inputs):\n",
    "    embeddings = []\n",
    "    for input in inputs:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = batch_embed(batched_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the embeddings\n",
    "output_file = \"embeddings_roberta.csv\"\n",
    "\n",
    "# Convert embeddings tensor to a numpy array\n",
    "embeddings_array = embeddings.numpy()\n",
    "\n",
    "# Write the embeddings to a CSV file\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for embedding_row in embeddings_array:\n",
    "        writer.writerow(embedding_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from the CSV file\n",
    "df = pd.read_csv(\"embeddings_roberta.csv\", header=None)\n",
    "embeddings = df.values\n",
    "\n",
    "# Create a BERTopic instance without specifying an embedding model\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit the topic model and get topics and probabilities\n",
    "topics, probabilities = topic_model.fit_transform(abstracts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x148cbe15d090>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.fit(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics().write_html('abstracts.html', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import re\n",
    "\n",
    "# Load the dataset again\n",
    "df = pd.read_csv('Web_of_Science_Query May 07 2024_1-5000.csv')\n",
    "abstracts = df['Abstract'].dropna().tolist()\n",
    "\n",
    "# Define a pre-processing function\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in ENGLISH_STOP_WORDS]  # Remove stop words\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Preprocess the abstracts\n",
    "abstracts = [preprocess(abstract) for abstract in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "# Again, load RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Function to tokenize text in batches\n",
    "def batch_tokenize(texts, batch_size=32):\n",
    "    all_inputs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        all_inputs.append(inputs)\n",
    "    return all_inputs\n",
    "\n",
    "# Function to generate embeddings for each batch\n",
    "def batch_embed(inputs):\n",
    "    embeddings = []\n",
    "    for input in inputs:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**input)\n",
    "            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(batch_embeddings)\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "batched_inputs = batch_tokenize(abstracts)\n",
    "embeddings = batch_embed(batched_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated embeddings\n",
    "output_file = \"embeddings_roberta_updated.csv\"\n",
    "\n",
    "# Convert this embeddings tensor to a numpy array\n",
    "embeddings_array = embeddings.numpy()\n",
    "\n",
    "# Write the new embeddings to a CSV file\n",
    "with open(output_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for embedding_row in embeddings_array:\n",
    "        writer.writerow(embedding_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"embeddings_roberta_updated.csv\", header=None)\n",
    "embeddings = df.values\n",
    "\n",
    "# Create a BERTopic instance without specifying an embedding model\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit the topic model and get topics and probabilities\n",
    "topics, probabilities = topic_model.fit_transform(abstracts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics().write_html('topics_update.html', auto_open=True) # Visualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf3_env]",
   "language": "python",
   "name": "conda-env-tf3_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
