{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#recode-analysis-of-environmental-literature-with-bertopic-and-roberta","title":"ReCoDE - Analysis of environmental literature with BERTopic and RoBERTa","text":""},{"location":"#explosive-literature-in-environmental-and-sustainability-studies","title":"Explosive literature in Environmental and Sustainability Studies","text":"<p>The field of environmental and sustainability studies has witnessed an explosive growth in literature over the past few decades, driven by the increasing global awareness and urgency surrounding environmental issues, climate change, and the need for sustainable practices.</p> <p>This rapidly expanding body of literature is characterized by its interdisciplinary nature, encompassing a wide range of disciplines such as ecology, climate science, energy, economics, policy, sociology, and more. With a global focus and contributions from countries around the world, the literature base reflects diverse cultural, socio-economic, and geographical contexts, often in multiple languages. Novel research areas and emerging topics, such as circular economy, sustainable urban planning, environmental justice, biodiversity conservation, renewable energy technologies, and ecosystem services, continue to arise as environmental challenges evolve and our understanding deepens. The development of environmental policies, regulations, and international agreements, as well as increased public interest and awareness, have further fueled research and the demand for literature aimed at informing and engaging various stakeholders. Technological advancements in areas like remote sensing, environmental monitoring, and computational modelling have enabled new avenues of research and data-driven studies, contributing to the proliferation of literature. The rise of open access publishing and digital platforms has facilitated the dissemination and accessibility of this constantly evolving and interdisciplinary body of knowledge.</p> <p>So, in summary, the explosive growth of the literature across multiple disciplines, geographic regions, languages, and emerging topics poses significant challenges in terms of effectively organizing, synthesizing, and extracting insights from this vast and rapidly expanding body of knowledge. This is where Natural Language Processing (NLP) techniques like topic modelling with BERTopic and advanced language models like RoBERTa can play a crucial role. Their ability to process large volumes of text data, identify semantic topics and patterns, cluster related documents, and handle multiple languages can help researchers, policymakers, and stakeholders navigate this extensive literature more effectively.</p> <p>Furthermore, as a STEMM PhD student at Imperial stepping into a new field such as Sustainability, taking advantage of the NLP tools can significantly enhance the efficiency of literature exploration and review. This skill facilitates a seamless transition into interdisciplinary research, empowering you to navigate diverse datasets and extract valuable insights with greater ease and precision.</p>"},{"location":"#the-potential-of-topic-modelling","title":"The Potential of Topic Modelling","text":"<p>Topic modelling is a technique in NLP and machine learning used to discover abstract \"topics\" that occur in a collection of documents. The key idea is that documents are made up of mixtures of topics, and that each topic is a probability distribution over words.</p> <p>More specifically, topic modelling algorithms like Latent Dirichlet Allocation (LDA) work by:</p> <ol> <li>Taking a set of text documents as input.</li> <li>Learning the topics contained in those documents in an unsupervised way. Each topic is represented as a distribution over the words that describe that topic.</li> <li>Assigning each document a mixture of topics with different weights/proportions.</li> </ol> <p>For example, if you ran topic modelling on a set of news articles, it may discover topics like \"politics\", \"sports\", \"technology\", etc. The \"politics\" topic would be made up of words like \"government\", \"election\", \"policy\" with high probabilities. Each document would then be characterized as a mixture of different proportions of these topics.</p> <p>The key benefits of topic modelling include:</p> <ol> <li>Automatically discovering topics without need for labeled data</li> <li>Understanding the themes/concepts contained in large document collections</li> <li>Organizing, searching, and navigating over a document corpus by topics</li> <li>Providing low-dimensional representations of documents based on their topics</li> </ol> <p>Topic modelling has found applications in areas like information retrieval, exploratory data analysis, document clustering and classification, recommendation systems, and more. Popular implementations include Latent Dirichlet Allocation (LDA), Biterm Topic Model (BTM), and techniques leveraging neural embeddings like BERTopic.</p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this tutorial, students will be able to acquire the following learning outcomes:</p> <ol> <li> <p>Proficiency in Text Data Preprocessing: Participants will gain hands-on experience in preprocessing environmental literature datasets, including cleaning, tokenisation, and normalisation techniques, essential for preparing data for NLP analysis.</p> </li> <li> <p>Understanding the principle of embedding-matrix-based NLP techniques: Through the application of BERTopic for topic modelling and RoBERTa for sentiment analysis, students will develop a deep understanding of advanced NLP methods and their practical implementation in dissecting environmental and sustainability texts and beyond.</p> </li> <li> <p>Critical Analysis Skills: Participants will learn to critically analyse and interpret the results of NLP analyses, including identifying dominant themes, sentiment shifts, and trends in environmental literature, fostering a nuanced understanding of environmental discourse.</p> </li> <li> <p>Interpretation and Application: Relying on a real-world example, this project demonstrates how to generate visualisations and reports to present the results of the topic modelling and sentiment analysis, facilitating interpretation and discussion.</p> </li> </ol>"},{"location":"#requirements","title":"Requirements","text":"<p>It would help a lot if you went through the following Graduate School courses before going through this exemplar: * Introduction to Python * Data Exploration and Visualisation * Data Processing with Python Pandas * Plotting in Python with Matplotlib * Binary Classification of Patent Text Using Natural Language Processing (another ReCoDE project)</p>"},{"location":"#academic","title":"Academic","text":"<ul> <li>Access to Google Colaboratory</li> <li>Basic Math (matrices, averages)</li> <li>Programming skills (python, pandas, numpy, tensorflow)</li> <li>Machine learning theory (at level of intro to machine learning course)</li> </ul>"},{"location":"#system","title":"System","text":"<p>Windows, MacOS, Ubuntu Python 3.11 or higher Ideally with GPU for fast running of the code</p> <p>NB: If you have access to High Performance Computing (HPC), we have prepared a specially adapted file for Imperial HPC environments, located under the \"notebook\" directory. This file is optimized to leverage the computational power and resources available through HPC, enabling more efficient processing and faster execution of your tasks.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#colab","title":"Colab","text":"<p>Please visit this Colab page to access the detailed content of this tutorial: https://colab.research.google.com/drive/1vJzmFTFurlK-NGDw_fhJgxSmcKSZooLn?usp=sharing</p> <p> </p>"},{"location":"#a-step-by-step-case-study-using-bertopic-to-analyze-one-web-of-science-dataset","title":"A Step-by-Step Case Study using BERTopic to Analyze One web of Science Dataset","text":"<p>In this step-by-step case study, we will focus on the application of BERTopic, to analyze a sample dataset sourced from Web of Science. Through this tutorial, we aim to guide you through the process:</p> <ul> <li>Installation and setup of BERTopic</li> <li>Collecting the raw data and preprocessing the dataset</li> <li>Implementing BERTopic for topic modeling</li> <li>Visualizing the inferred topics and interpreting the results</li> <li>Fine-tuning topic representations</li> <li>Additional readings about the wider application of BERTopic</li> </ul> <p>By following along, you will gain practical insights into leveraging BERTopic for insightful analysis of scholarly literature from Web of Science.</p> <p>Some sample visualisation results can be:  </p>"},{"location":"#a-step-by-step-case-study-using-roberta","title":"A Step-by-Step Case Study using RoBERTa","text":"<p>Similar to what we have done above, we need to follow the following steps when applying a RoBERTa model.</p> <ul> <li>RoBERTa Initialization: Initializes RoBERTa tokenizer and model</li> <li>Data Preparation: Loads and preprocesses the dataset</li> <li>Batch Tokenization: Tokenizes abstracts in batches</li> <li>Embedding Generation: Generates embeddings using RoBERTa, and save it</li> <li>Topic Modeling: Applies BERTopic with RoBERTa embeddings</li> <li>Improve and fine-tune</li> <li>Visualization</li> </ul> <p>This section focuses on integrating RoBERTa into the topic modeling pipeline, enhancing its analytical capabilities.</p> <p>Some sample visualisation results can be:  </p>"},{"location":"#frequently-asked-questions-and-best-practices-all-details-can-be-found-on-the-colab-page","title":"Frequently Asked Questions and Best Practices (All details can be found on the Colab page)","text":"<p>https://colab.research.google.com/drive/1vJzmFTFurlK-NGDw_fhJgxSmcKSZooLn#scrollTo=SbhVeLI72Idj&amp;line=1&amp;uniqifier=1 </p> <ol> <li>Specify a preferred hardware accelerator on Colab</li> <li>Besides Web of Science, where else can I find datasets, and how can I import literature datasets in bulk across platforms?</li> <li>Can I publish the textual dataset I pre-processed, and where\uff1f</li> <li>Why do we need to explore literature and compared to manual exploration, what are the advantages of applying models like BERT?</li> <li>Factors helping you decide whether to apply RoBERTa (if you have used a general BERToic model):</li> <li>Why is preprocessing necessary when using RoBERTa but not always required when using BERTopic\uff1f</li> <li>Why is it necessary to pretrain a RoBERTa model?</li> <li>What happens next after pretraining?</li> </ol>"},{"location":"#further-reading","title":"Further Reading","text":"<ul> <li>The BERTopic developer's github page: https://github.com/MaartenGr/BERTopic</li> <li>The BERTopic developer's personal blog page: https://www.maartengrootendorst.com/</li> <li>Tutorial page - Leveraging BERT and c-TF-IDF to create easily interpretable topics: maartengr.github.io/BERTopic/</li> <li>Natural Language Processing: A Textbook with Python Implementation (by Raymond S. T. Lee): https://www.amazon.co.uk/Natural-Language-Processing-Textbook-Implementation-ebook/dp/B0CBR29GV2</li> <li>Speech and Language Processing (3rd ed. draft) (by Dan Jurafsky and James H. Martin): https://web.stanford.edu/~jurafsky/slp3/</li> <li>Multi-lingual and multi-modal topic modelling with pre-trained embeddings: https://aclanthology.org/2022.coling-1.355.pdf</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>.\n\u251c\u2500\u2500 .github                   &lt;- github Actions workflows\n\u2502\n\u251c\u2500\u2500 docs                      &lt;- documents\n\u2502   \u251c\u2500\u2500 .icons/logos             &lt;- icon\n\u2502   \u251c\u2500\u2500 datasets                 &lt;- raw datasets\n\u2502       \u2502\u2500\u2500 Web_of_Science_Query May 07 2024_1-5000.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_1-1000 results.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_1001-2000 results.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_2001-3000 results.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_3001-4000 results.csv\n\u2502       \u2502\u2500\u2500 Web_of_Science_Search_4001-5000 results.csv\n\u2502   \u251c\u2500\u2500 plots                 &lt;- plots\n\u2502       \u2502\u2500\u2500 hier_cluster_bert_1.png\n\u2502       \u2502\u2500\u2500 hier_cluster_roberta_1.png\n\u2502       \u2502\u2500\u2500 hier_cluster_roberta_2.png\n\u2502       \u2502\u2500\u2500 inter-topic_bert1.png\n\u2502       \u2502\u2500\u2500 inter-topic_roberta1.png\n\u2502       \u2502\u2500\u2500 inter-topic_roberta2.png\n\u2502       \u2502\u2500\u2500 sim_max_bert_1.png\n\u2502       \u2502\u2500\u2500 sim_max_roberta_2.png\n\u2502       \u2502\u2500\u2500 top_topics_bert_1.png\n\u2502       \u2502\u2500\u2500 top_topics_roberta_2.png\n\u2502   \u251c\u2500\u2500 BERT_Walkthrough.md\n\u2502   \u251c\u2500\u2500 CARBON.md\n\u2502   \u251c\u2500\u2500 FAQ.md\n\u2502   \u251c\u2500\u2500 Readings.md\n\u2502   \u251c\u2500\u2500 RoBERTa_Walkthrough.md\n\u2502   \u251c\u2500\u2500 data_sources.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 key_concepts.md\n\u251c\u2500\u2500 notebooks                 &lt;- project data\n\u2502       \u251c\u2500\u2500 ReCoDE_Analysis_of_environmental_literature_with_BERTopic_and_RoBERTa_colab.ipynb          &lt;- Saved .ipynb file from colab\n\u2502       \u251c\u2500\u2500 ReCoDE-BERTopic&amp;RoBERTa_Run on HPC at Imperial.ipynb                                 &lt;- Saved .ipynb file suitable for running on High-Performance Computer\n\u251c\u2500\u2500 mkdocs.yml                \n\u251c\u2500\u2500 requirements.txt          &lt;- file for installing python dependencies\n\u251c\u2500\u2500 LICENSE.md\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the BSD-3-Clause license</p>"},{"location":"BERT_Walkthrough/","title":"A Step-by-Step Case Study using BERTopic to Analyze One web of Science Dataset","text":"<p>In this step-by-step case study, we will focus on the application of BERTopic, to analyze a sample dataset sourced from Web of Science. Through this tutorial, we aim to guide you through the process:</p> <ul> <li>Installation and setup of BERTopic</li> <li>Collecting the raw data and preprocessing the dataset</li> <li>Implementing BERTopic for topic modeling</li> <li>Visualizing the inferred topics and interpreting the results</li> <li>Fine-tuning topic representations</li> <li>Additional readings about the wider application of BERTopic</li> </ul> <p>By following along, you will gain practical insights into leveraging BERTopic for insightful analysis of scholarly literature from Web of Science.</p>"},{"location":"BERT_Walkthrough/#data-preparation","title":"Data Preparation","text":""},{"location":"BERT_Walkthrough/#load-the-dataset-preview-the-data","title":"Load the dataset &amp; Preview the data","text":"<pre><code>df = pd.read_csv(\"Web_of_Science_Query May 07 2024_1-5000.csv\", encoding='utf-8')\nprint(df.head())\n</code></pre>"},{"location":"BERT_Walkthrough/#data-analysis","title":"Data Analysis","text":""},{"location":"BERT_Walkthrough/#applying-bert-and-validation","title":"Applying BERT and Validation","text":"<p>The provided code snippet employs the BERTopic library to conduct topic modeling on the given dataset \"Web_of_Science_Query May 07 2024_1-5000.csv\". Initially, a BERTopic instance is created, enabling the implementation of BERT-based topic modeling. Subsequently, the model is trained on the dataset by fitting it with the abstracts extracted from the DataFrame 'df'. The 'Abstract' column is accessed and converted into a list of documents ('docs'), which serves as the input data for the topic modeling process. As a result, the code generates topics and corresponding probabilities for each document, facilitating the extraction of meaningful themes and insights from the dataset.</p> <pre><code>from bertopic import BERTopic\nimport pandas as pd\n</code></pre>"},{"location":"BERT_Walkthrough/#load-your-dataframe-with-the-abstracts","title":"Load your DataFrame with the abstracts","text":"<pre><code>df = pd.read_csv(\"Web_of_Science_Query May 07 2024_1-5000.csv\", encoding='utf-8')\n</code></pre>"},{"location":"BERT_Walkthrough/#preprocess-the-data-to-handle-null-values","title":"Preprocess the data to handle null values","text":"<pre><code>df['Abstract'] = df['Abstract'].fillna('')  # Replace null values with empty strings\n</code></pre>"},{"location":"BERT_Walkthrough/#create-a-bertopic-instance","title":"Create a BERTopic instance","text":"<pre><code>topic_model = BERTopic(verbose=True)\n</code></pre>"},{"location":"BERT_Walkthrough/#fit-the-model-on-your-dataset","title":"Fit the model on your dataset","text":"<pre><code>docs = df['Abstract'].tolist()\ntopics, probs = topic_model.fit_transform(docs)\n</code></pre> <p>Before we start visualizing the results, a good approach to double-check whether the topics have been successfully inferred by the BERTopic model is listed below. The code is checking whether the topics_ attribute exists and is not None in the topic_model object. If the attribute exists and is not None, it prints the inferred topics; otherwise, it prints a message indicating that the topics_ attribute has not been populated yet.</p> <p>By doing so, we can prevent errors and ensure that we have valid topic data to work with.</p>"},{"location":"BERT_Walkthrough/#check-if-the-topics_-attribute-exists-and-is-not-none","title":"Check if the topics_ attribute exists and is not None","text":"<pre><code>if hasattr(topic_model, 'topics_') and topic_model.topics_ is not None:\n    # Print the inferred topics\n    print(\"Inferred Topics:\")\n    print(topic_model.topics_)\nelse:\n    print(\"The topics_ attribute has not been populated yet.\")\n</code></pre>"},{"location":"BERT_Walkthrough/#number-of-documents","title":"Number of documents","text":"<p>In many cases, especially in text analysis tasks like topic modeling, each record or row in the dataset corresponds to a single document, especially when the dataset is highly structured. For example, in this DataFrame where each row represents a different research paper, then the 'Abstract' column in each row contains the abstract of that paper - each record in the DataFrame contains unique textual content. In this case, there is a one-to-one mapping between documents and records, where each document corresponds to a single record in the DataFrame.</p> <p>To determine the number of documents in this demo dataset, you can use the len() function in Python, which returns the length of a list or the number of elements in an object.</p> <pre><code>num_documents = len(docs)\nprint(\"Number of documents:\", num_documents)\n</code></pre> <p>If the dataset is a plain text file with huge chunks of text, determining the number of documents can be more challenging as there may not be a clear separation between individual documents. However, you can use various techniques to identify and count the number of documents in the text file.</p> <p>Try to identify a pattern or delimiter that separates individual documents in the text file. This could be a specific string, a sequence of characters, or a blank line. For example, if each document starts with a line like \"Document #123\", you can use that as the delimiter, and use the split() method with the defined delimiter to split the text into a list of individual documents.</p>"},{"location":"BERT_Walkthrough/#embedding","title":"Embedding","text":"<p>In the following code snippet, we are generating and preserving embeddings \u2014 a numerical representation of textual data (see in \"key concepts\" section).</p> <p>Utilizing the <code>sentence_transformers</code> library, we begin by initializing a SentenceTransformer model called 'all-MiniLM-L6-v2'. This model serves as our guide in converting text into dense numerical vectors known as embeddings. With the model ready, we proceed to create a BERTopic model, which leverages the embeddings for topic modeling. We then apply our SentenceTransformer model to encode a list of documents into embeddings.</p> <p>These embeddings are saved in various formats\u2014a NumPy array and a pickle file\u2014for future analysis or reuse. Additionally, we convert the embeddings into a pandas DataFrame and export them to a CSV file, facilitating easy access and manipulation of the data.</p> <p>However, why do we need to save the embeddings? Because it allows for efficient storage and retrieval of numerical representations of textual data, facilitating reproducibility and consistency in downstream tasks. It also serves as a backup or checkpointing mechanism for data processing pipelines, enhancing the efficiency and robustness of analyses and experiments.</p>"},{"location":"BERT_Walkthrough/#import-the-sentencetransformer-library-to-access-pre-trained-models-for-generating-sentence-embeddings","title":"Import the SentenceTransformer library to access pre-trained models for generating sentence embeddings","text":"<pre><code>from sentence_transformers import SentenceTransformer\n</code></pre>"},{"location":"BERT_Walkthrough/#initialize-a-sentencetransformer-model-with-the-all-minilm-l6-v2-variant-for-generating-embeddings","title":"Initialize a SentenceTransformer model with the 'all-MiniLM-L6-v2' variant for generating embeddings","text":"<pre><code>embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n</code></pre>"},{"location":"BERT_Walkthrough/#initialize-a-bertopic-model-with-the-specified-sentencetransformer-embedding-model-and-enable-verbose-mode-for-logging","title":"Initialize a BERTopic model with the specified SentenceTransformer embedding model and enable verbose mode for logging","text":"<pre><code>topic_model = BERTopic(embedding_model=embedding_model, verbose=True)\n</code></pre>"},{"location":"BERT_Walkthrough/#encode-the-list-of-documents-into-embeddings-using-the-initialized-sentencetransformer-model-showing-a-progress-bar-during-the-encoding-process","title":"Encode the list of documents into embeddings using the initialized SentenceTransformer model, showing a progress bar during the encoding process","text":"<pre><code>embeddings = embedding_model.encode(docs, show_progress_bar=True)\n</code></pre>"},{"location":"BERT_Walkthrough/#save-the-embeddings-to-a-numpy-array-file-npy","title":"Save the embeddings to a NumPy array file (.npy)","text":"<pre><code>import numpy as np\nnp.save('embeddings.npy', embeddings)  # Save to .npy file\n</code></pre>"},{"location":"BERT_Walkthrough/#save-the-embeddings-to-a-pickle-file-for-serialization-pkl","title":"Save the embeddings to a pickle file for serialization (.pkl)","text":"<p>Serialization refers to the process of converting an object into a format that can be easily stored, transmitted, or reconstructed later. In Python, serialization is commonly used for saving objects to files or transferring them between different systems. The .pkl extension here denotes a pickle file, which is a binary file format used for serializing and deserializing objects. Pickle files can store various Python objects, such as lists, dictionaries, and even custom classes, in a compact and efficient binary format.</p> <pre><code>import pickle\nwith open('embeddings.pkl', 'wb') as file:\n    pickle.dump(embeddings, file)\n</code></pre>"},{"location":"BERT_Walkthrough/#convert-the-embeddings-into-a-pandas-dataframe-for-further-analysis-and-export-it-to-a-csv-file-without-indexing","title":"Convert the embeddings into a pandas DataFrame for further analysis and export it to a CSV file without indexing","text":"<pre><code>import pandas as pd\nembeddings_df = pd.DataFrame(embeddings)\nembeddings_df.to_csv('embeddings.csv', index=False)\n</code></pre>"},{"location":"BERT_Walkthrough/#visulizations-and-interpretation","title":"Visulizations and Interpretation","text":"<p>It is difficult to (mathematically) define interpretability (Molnar 2022). A (non-mathematical) definition of interpretability that I like by Miller (2017) is: Interpretability is the degree to which a human can understand the cause of a decision. Another one by Kim et al. (2016) is: Interpretability is the degree to which a human can consistently predict the model\u2019s result.</p> <p>Through visualizations and explanations of these outputs, I aim to guide you through a process that you can gain insights into how the model makes decisions and understand the factors influencing its predictions.</p>"},{"location":"BERT_Walkthrough/#intertopic-distance-map","title":"Intertopic Distance Map","text":"<p>The <code>visualize_topics()</code> method in BERTopic is used to generate visualizations of the inferred topics. It provides a graphical representation that allows you to explore the topics and their associated keywords in an intuitive way.</p> <p>When you call <code>topic_model.visualize_topics()</code>, it generates a visualization that typically includes:</p> <ol> <li> <p>Topic Clusters: Topics are often represented as clusters, with each cluster containing multiple related topics. These clusters can help you identify overarching themes or categories within the dataset.</p> </li> <li> <p>Keyword Distribution: For each topic cluster, the visualization typically displays the keywords that are most strongly associated with each topic. These keywords give you insights into the main concepts and ideas represented by each topic.</p> </li> <li> <p>Topic Distribution: The visualization may also include information about the distribution of documents across topics. This can help you understand how prevalent each topic is in the dataset and how topics relate to one another. topic_model.visualize_topics()</p> </li> </ol>"},{"location":"BERT_Walkthrough/#topic-word-scores","title":"Topic Word Scores","text":"<p>The <code>visualize_barchart()</code> method in BERTopic generates a bar chart visualization of the most prominent topics based on their prevalence in the document corpus.</p> <p>When you call <code>topic_model.visualize_barchart(top_n_topics=15)</code>, it generates a bar chart that typically includes:</p> <ol> <li> <p>Topic Distribution: The bar chart displays the distribution of documents across the top N topics, where N is specified by the <code>top_n_topics</code> parameter. Each bar represents a topic, and the height of the bar indicates the proportion of documents assigned to that topic.</p> </li> <li> <p>Topic Labels: The topics are usually labeled along the x-axis of the bar chart, allowing you to identify each topic.</p> </li> <li> <p>Document Counts: The y-axis of the bar chart typically represents the number of documents assigned to each topic, providing insight into the prevalence of each topic in the document corpus.</p> </li> </ol> <p>Deciding the value of <code>n</code> for topic modeling involves considering domain knowledge, conducting exploratory analysis, and evaluating model performance. It is essential to balance granularity and interpretability, aiming for a value that produces meaningful topics. Experimenting with different <code>n</code> values and assessing the coherence and relevance of the generated topics can help in making an informed decision. topic_model.visualize_barchart(top_n_topics=15)</p>"},{"location":"BERT_Walkthrough/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<p>The visualize_hierarchy method in topic_model is used to create a hierarchical visualization of topics, with the top 100 topics being displayed. This visualization helps in understanding the relationships and hierarchical structure between topics, providing insights into how topics are grouped and nested within the document corpus.</p> <pre><code>topic_model.visualize_hierarchy(top_n_topics=100)\n</code></pre>"},{"location":"BERT_Walkthrough/#similarity-matrix","title":"Similarity Matrix","text":"<p>The <code>visualize_heatmap</code> method in <code>topic_model</code> generates a heatmap visualization of the topic-document matrix, highlighting the distribution of topics across documents. With <code>top_n_topics</code> set to <code>100</code>, the heatmap displays the top 100 topics and their prevalence within the document corpus. This visualization aids in understanding the relative importance and coverage of different topics, offering insights into the thematic composition of the dataset and identifying potential patterns or trends.</p> <pre><code>topic_model.visualize_heatmap(top_n_topics=100)\n</code></pre>"},{"location":"BERT_Walkthrough/#fine-tune-topic-representations","title":"Fine-tune Topic Representations","text":"<p>To fine-tune topic representations, you can explore several strategies:</p> <ol> <li> <p>Adjusting Model Hyperparameters: Experiment with different hyperparameters of the topic modeling algorithm, such as the number of topics (<code>n_topics</code>), the vectorization method, or the dimensionality reduction technique. Tuning these parameters can affect the quality and granularity of the inferred topics.</p> </li> <li> <p>Optimizing Text Preprocessing: Refine the preprocessing steps applied to the text data before topic modeling. This may involve techniques such as tokenization, stemming, lemmatization, or removing stop words. Fine-tuning preprocessing can enhance the quality of topic representations by reducing noise and improving semantic coherence.</p> </li> <li> <p>Incorporating Domain Knowledge: Incorporate domain-specific knowledge or constraints into the topic modeling process. This can be achieved by providing seed words or phrases related to specific topics of interest or by constraining the model to generate topics within predefined thematic boundaries.</p> </li> <li> <p>Ensemble Modeling: Explore ensemble modeling techniques, where multiple topic modeling algorithms or variations of the same algorithm are combined to improve topic representations. Ensemble methods can mitigate the limitations of individual models and enhance the robustness of topic inference.</p> </li> <li> <p>Evaluation and Iteration: Continuously evaluate the quality of topic representations using domain-specific metrics or qualitative assessments. Iterate on the fine-tuning process based on feedback and insights gained from analyzing the topics generated by the model.</p> </li> </ol>"},{"location":"BERT_Walkthrough/#additional-outcomes-to-explore","title":"Additional Outcomes to Explore","text":"<p>In addition to the outcomes we introduced above, you could also consider exploring the following outcomes if time and resources allow:</p> <ul> <li> <p>Topic Evolution Over Time: Analyze how topics evolve over time. This can be particularly useful if your dataset spans several years. You can also identify trends and shifts in research focus.</p> </li> <li> <p>Institutional or Geographic Distribution: Examine how topics vary across different institutions or geographic regions.</p> </li> <li> <p>Sentiment Analysis: Apply sentiment analysis within each topic to understand the sentiment trends related to different research themes.</p> </li> <li> <p>Topic Diversity and Distribution: Calculate metrics such as topic diversity or entropy to understand how spread out the topics are across the papers.</p> </li> <li> <p>Topic Coherence and Perplexity: Evaluate the coherence and perplexity of the topics to quantitatively measure their quality.</p> </li> </ul>"},{"location":"BERT_Walkthrough/#wider-applications-of-bert","title":"Wider applications of BERT","text":""},{"location":"BERT_Walkthrough/#example-1-embedding-backend-for-the-bertopic-library","title":"Example 1 - Embedding backend for the BERTopic library","text":"<p>The following command installs BERTopic along with additional dependencies for different embedding backends. Here's a brief explanation of each backend:</p> <ul> <li> <p>Flair: Flair is a natural language processing library that provides contextual string embeddings. It offers state-of-the-art pre-trained models for various NLP tasks.</p> </li> <li> <p>Gensim: Gensim is a Python library for topic modeling, document similarity analysis, and other natural language processing tasks. It includes algorithms for word embedding models like Word2Vec and Doc2Vec.</p> </li> <li> <p>Spacy: Spacy is a powerful and efficient library for natural language processing in Python. It provides pre-trained word vectors and other NLP functionalities.</p> </li> <li> <p>USE (Universal Sentence Encoder): USE is a deep learning model developed by Google for generating universal sentence embeddings. It encodes sentences into fixed-length dense vectors that capture semantic information.</p> </li> </ul> <p>By installing BERTopic with these additional dependencies, you gain access to multiple embedding options, allowing you to choose the one that best suits your needs for topic modeling and related tasks. pip install bertopic[flair, gensim, spacy, use]</p>"},{"location":"BERT_Walkthrough/#example-2-beit-topic-modelling-with-images","title":"Example 2 - BEiT: Topic modelling with images","text":"<p>BEiT (BERT Pre-Training of Image Transformers) is a novel approach to pre-training image transformers, inspired by the success of BERT (Bidirectional Encoder Representations from Transformers) in natural language processing tasks. Unlike traditional convolutional neural networks (CNNs) commonly used for image processing, BEiT leverages transformer architectures to learn representations of images. It represents a promising direction in the field of computer vision, offering a new perspective on image representation learning and paving the way for advancements in various image-related tasks.</p> <p>If you are interested in reading more about this, please visit this link: https://arxiv.org/pdf/2106.08254</p>"},{"location":"CARBON/","title":"Be Sensible about the Carbon Intensivity when Running your Code","text":"<p>When employing BERTopic or RoBERTa, it is essential to recognize the broader implications of model selection on environmental sustainability. While BERTopic offers a powerful approach to uncovering latent topics within textual data, the choice of underlying transformer model, significantly impacts computational resources and energy consumption.</p> <p>Especially RoBERTa, known for its superior performance in natural language understanding tasks, demands substantial computational power during training and inference stages, thereby contributing to higher carbon emissions. As such, developers leveraging BERTopic with RoBERTa should prioritize optimization strategies to mitigate the environmental impact of their code. This may entail fine-tuning model parameters, employing efficient batch processing techniques, or exploring alternative models with lower carbon intensities.</p> <p>Furthermore, it is advisable to preserve your embeddings. By doing so, you eliminate the need to rerun the entire analysis each time, reducing energy expenditure and contributing to a more sustainable workflow.</p>"},{"location":"FAQ/","title":"FAQ","text":"<p>1. Specify a preferred hardware accelerator on Colab</p> <p>CPU (Central Processing Unit), GPU (Graphics Processing Unit), and TPU (Tensor Processing Unit) are different types of processors designed for different computing tasks.</p> <p>CPUs are general-purpose processors suitable for a wide range of computing tasks, GPUs are specialized for parallel processing and are widely used for accelerating machine learning and scientific computations, while TPUs are custom-designed by Google specifically for machine learning workloads and are primarily used in data centers and cloud environments.</p> <p>In this tutorial, I have set CPU as a default.</p> <p>What types of GPU/TPUs are available in Colab?</p> <p>The types of GPUs and TPUs that are available in Colab vary over time. This is necessary for Colab to be able to provide access to these resources free of charge.</p> <p>We have two TPU types: \"TPU (deprecated)\" and \"TPU v2\". \"TPU (deprecated)\" is backed by a TPU Node architecture system. On TPU Node architecture systems, the TPU is hosted on a remote machine. All TPU operations are sent over the network. This can introduce performance problems and debugging difficulties. \"TPU v2\" has a TPU VM architecture, where the TPU is attached to the local VM. The \"TPU v2\" backend has a 4-chip v2 TPU.</p> <p>You can access premium GPUs subject to availability by purchasing one of our paid plans here.</p> <p>If you would like access to specific dedicated hardware, explore using GCP Marketplace Colab.</p> <p>To specify the preferred hardware accelerator on Google Colab, follow these steps:</p> <ol> <li> <p>Open Google Colab: Go to Google Colab.</p> </li> <li> <p>Create a new notebook or open an existing one.</p> </li> <li> <p>Go to the \"Runtime\" menu at the top of the Colab interface.</p> </li> <li> <p>Select \"Change runtime type\" from the dropdown menu. In the dialog that appears, you can choose the hardware accelerator you prefer:</p> </li> <li>For a GPU, select \"GPU\" from the \"Hardware accelerator\" dropdown.</li> <li>For a TPU, select \"TPU\" from the \"Hardware accelerator\" dropdown. Click \"Save\" to apply the changes.</li> </ol> <p>2. Besides Web of Science, where else can I find datasets, and how can I import literature datasets in bulk across platforms?</p> <p>There are many different sources. Here are some examples:</p> <ol> <li> <p>ArXiv Dataset - ArXiv dataset and metadata of 1.7M+ scholarly papers across STEM: For nearly 30 years, ArXiv has served the public and research communities by providing open access to scholarly articles, from the vast branches of physics to the many subdisciplines of computer science to everything in between, including math, statistics, electrical engineering, quantitative biology, and economics. This rich corpus of information offers significant, but sometimes overwhelming depth. It is a collaboratively funded, community-supported resource founded by Paul Ginsparg in 1991 and maintained and operated by Cornell University. In these times of unique global challenges, efficient extraction of insights from data is essential.</p> </li> <li> <p>Kaggle: To help make the ArXiv more accessible, a free, open pipeline has been presented on Kaggle to the machine-readable ArXiv dataset: a repository of 1.7 million articles, with relevant features such as article titles, authors, categories, abstracts, full text PDFs, and more. The hope is to empower new use cases that can lead to the exploration of richer machine learning techniques that combine multi-modal features towards applications like trend analysis, paper recommender engines, category prediction, co-citation networks, knowledge graph construction and semantic search interfaces. Therefore, it can be a good data source. There are other datasets on Kaggle too.</p> </li> <li> <p>Semantic Scholar: The ArXiv dataset is rather large (1.1TB and growing). It is an AI-powered academic search engine that helps researchers discover and access scientific literature more efficiently. It offers advanced search capabilities, personalized recommendations, and tools for analyzing research papers. To retrieve papers with specific search criteria like \"Environmental Science\", date range from \"2004-2024\", \"Has PDF\", and from Journals &amp; Conference as \"arXiv.org\" without specific keywords, and you can use API to retrieve datasets.</p> </li> </ol> <p>3. Can I publish the textual dataset I pre-processed, and where\uff1f</p> <p>Publishing your curated text dataset for future NLP model training is a great way to contribute to the research community and support the development of new models. There are several platforms where you can publish and share your dataset, for instance:</p> <ol> <li> <p>Kaggle: Kaggle is a well-known platform for data science competitions and datasets. It has a large community of data scientists and researchers. Link: Kaggle Datasets</p> </li> <li> <p>Zenodo: Zenodo is an open-access repository developed by CERN. It allows researchers to share datasets, papers, and other research outputs. Zenodo also assigns a DOI (Digital Object Identifier) to your dataset for easy citation. Link: Zenodo</p> </li> <li> <p>GitHub: GitHub is a popular platform for hosting code repositories. It can also be used to share datasets, especially if they are relatively small or if you want to include code for data processing. When creating a new repository and upload your dataset files. Remember to add a README file with details about the dataset. Link: GitHub</p> </li> </ol> <p>4. Why do we need to explore literature and compared to manual exploration, what are the advantages of applying models like BERT?</p> <p>Exploring literature allows researchers to uncover new insights and understand trends within a field. By systematically reviewing what has been written, you can identify gaps in knowledge, emerging themes, and potential areas for further research. This process is crucial for advancing any scientific discipline. The rapid pace of academic publishing means that staying current with the latest developments is a daunting task. Literature exploration helps keep researchers up-to-date with the newest findings and theories, ensuring that their work remains relevant and informed by the latest evidence.</p> <p>Using models like BERT offers significant advantages, such as efficiency and speed, where BERT acts like a highly skilled assistant, automating the processing of large text volumes and allowing researchers to focus on interpretation rather than data handling. Its depth and precision enable a nuanced understanding of language, accurately identifying and categorizing topics beyond simple keyword methods. BERT ensures consistency and reduces biases inherent in human analysis, providing a more objective and reliable outcome. It also scales efficiently to handle extensive datasets, offering timely insights regardless of size. Additionally, BERT can uncover hidden patterns and trends in literature, aiding strategic planning, and supports multilingual analysis, making it invaluable in global research contexts by breaking down language barriers for a comprehensive review.</p> <p>5. Factors helping you decide whether to apply RoBERTa (if you have used a general BERToic model):</p> <p>Quality of Topics: Evaluate the coherence and relevance of the topics generated by BERTopic. If the topics are not very coherent or do not align well with your domain knowledge, applying RoBERTa might improve the results as it can capture more nuanced language features.</p> <p>Comparison and Validation: If you want to validate and compare the robustness of the topics, applying RoBERTa can provide a benchmark. Comparing the results can highlight strengths and weaknesses of both models.</p> <p>Computational Resources: Applying RoBERTa is computationally intensive. Ensure you have the necessary resources to run and compare the models.</p> <p>Specific Research Questions: If your research questions require highly refined topics or involve understanding subtleties in language that BERTopic might miss, then using RoBERTa could be beneficial.</p> <p>6. Why is preprocessing necessary when using RoBERTa but not always required when using BERTopic\uff1f</p> <p>BERTopic is a topic modeling technique that often combines pre-trained embedding models (like BERT, RoBERTa, etc.) with clustering algorithms. BERTopic can take raw text inputs directly and generate topics. The default embedding models used by BERTopic, such as those from sentence-transformers, are generally robust to common noise (like stop words) and can still produce meaningful topics even without extensive preprocessing. This built-in robustness means that BERTopic can sometimes handle raw text better without extensive cleaning.</p> <p>RoBERTa, on the other hand, is a transformer-based language model that processes text at a more granular level. When using RoBERTa directly for tasks like generating embeddings, the presence of noise, such as stop words, punctuation, and irrelevant characters, can impact the quality of the embeddings. RoBERTa does not inherently ignore or downplay stop words and other noise. Therefore, preprocessing steps such as removing stop words, converting text to lowercase, and eliminating punctuation can help improve the quality of the embeddings produced by RoBERTa, leading to better downstream analysis. So, preprocessing text before using RoBERTa can significantly improve the quality of your topic modeling results by reducing the impact of irrelevant words and noise.</p> <p>7. Why is it necessary to pretrain a RoBERTa model?</p> <p>Pretraining a RoBERTa model is necessary because it allows the model to learn rich, general-purpose language representations from large amounts of text data. During pretraining, RoBERTa is exposed to vast corpora of text, where it learns to understand the nuances of language, capture contextual information, and encode semantic meaning into its embeddings. This pretraining process enables RoBERTa to capture a wide range of linguistic patterns and structures, making it a powerful tool for various natural language processing tasks.</p> <p>8. What happens next after pretraining?</p> <p>After pretraining, the next step is fine-tuning the RoBERTa model on your specific downstream task. Fine-tuning involves taking the pretrained RoBERTa model and adapting it to perform well on your specific dataset and task. During fine-tuning, you feed your task-specific data (e.g., labeled text for sentiment analysis, question-answer pairs for question answering) into the pretrained RoBERTa model and update its parameters through backpropagation. This process allows RoBERTa to adjust its learned representations to better suit the intricacies of your particular task, ultimately leading to improved performance and better results. Fine-tuning typically involves training the model on a smaller dataset specific to your task, which helps it learn task-specific patterns and nuances. Once fine-tuning is complete, you can use the fine-tuned RoBERTa model for inference and predictions on new, unseen data in your specific domain or application.</p>"},{"location":"Readings/","title":"Readings","text":"<ol> <li> <p>The BERTopic developer's github page: https://github.com/MaartenGr/BERTopic</p> </li> <li> <p>The BERTopic developer's personal blog page: https://www.maartengrootendorst.com/</p> </li> <li> <p>Tutorial page - Leveraging BERT and c-TF-IDF to create easily interpretable topics: maartengr.github.io/BERTopic/</p> </li> <li> <p>Natural Language Processing: A Textbook with Python Implementation (by Raymond S. T. Lee): https://www.amazon.co.uk/Natural-Language-Processing-Textbook-Implementation-ebook/dp/B0CBR29GV2</p> </li> <li> <p>Speech and Language Processing (3rd ed. draft) (by Dan Jurafsky and James H. Martin): https://web.stanford.edu/~jurafsky/slp3/</p> </li> <li> <p>Multi-lingual and multi-modal topic modelling with pre-trained embeddings: https://aclanthology.org/2022.coling-1.355.pdf</p> </li> </ol>"},{"location":"RoBERTa_Walkthrough/","title":"A Step-by-Step Case Study using RoBERTa","text":"<p>Similart to what we have done above, we need to follow the following steps when applying a RoBERTa model.</p> <ul> <li>RoBERTa Initialization: Initializes RoBERTa tokenizer and model</li> <li>Data Preparation: Loads and preprocesses the dataset</li> <li>Batch Tokenization: Tokenizes abstracts in batches</li> <li>Embedding Generation: Generates embeddings using RoBERTa, and save it</li> <li>Topic Modeling: Applies BERTopic with RoBERTa embeddings</li> <li>Improve and fine-tune</li> <li>Visualization</li> </ul> <p>This section focuses on integrating RoBERTa into the topic modeling pipeline, enhancing its analytical capabilities.</p>"},{"location":"RoBERTa_Walkthrough/#dataset","title":"Dataset","text":"<p>We will be using the same dataset, \"Web_of_Science_Query May 07 2024_1-5000.csv\".</p> <pre><code>import pandas as pd\n\n# Load dataset\ndf = pd.read_csv('Web_of_Science_Query May 07 2024_1-5000.csv', encoding='utf-8')\nabstracts = df['Abstract'].dropna().tolist()  # Ensure no NaN values\n\n# Ensure all elements are strings\nabstracts = [str(abstract) for abstract in abstracts]\n\n# Debug: Print the first few elements to check\nprint(abstracts[:5])\n</code></pre>"},{"location":"RoBERTa_Walkthrough/#tokenize-the-data","title":"Tokenize the Data","text":"<p>Convert the abstracts into tokens that the RoBERTa model can process.</p> <pre><code># Function to tokenize in batches\ndef batch_tokenize(texts, batch_size=32):\n    all_inputs = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)\n        all_inputs.append(inputs)\n    return all_inputs\n\n# Tokenize abstracts in batches\nbatched_inputs = batch_tokenize(abstracts)\n</code></pre>"},{"location":"RoBERTa_Walkthrough/#embedding-generation","title":"Embedding Generation","text":"<p>This following part is responsible for generating embeddings for each batch of tokenized inputs. More specifically:</p> <ul> <li><code>inputs</code>: This parameter represents a list of tokenized inputs. Each element in the list corresponds to a batch of tokenized input data.</li> <li><code>embeddings = []</code>: This initializes an empty list to store the embeddings generated for each batch.</li> <li>Batch Processing: The function iterates through each batch of tokenized inputs provided in the <code>inputs</code> list. Within each iteration, a <code>with torch.no_grad():</code> block ensures that no gradients are calculated during the forward pass, reducing memory consumption and speeding up computations.</li> <li><code>outputs = model(**input)</code>: This line feeds the current batch of tokenized inputs (<code>input</code>) to the RoBERTa model (<code>model</code>) to obtain the model outputs.</li> <li><code>outputs.last_hidden_state</code>: The <code>outputs</code> object contains various attributes, including the last hidden states of all tokens in the input sequence. Here, <code>last_hidden_state</code> retrieves these hidden states.</li> <li><code>batch_embeddings = outputs.last_hidden_state.mean(dim=1)</code>: This computes the mean of the last hidden states along the sequence dimension (dimension 1), resulting in a single vector representation (embedding) for each input sequence in the batch.</li> <li><code>torch.cat(embeddings)</code>: Finally, all the embeddings generated for different batches are concatenated along the batch dimension (dimension 0) using PyTorch's <code>torch.cat()</code> function, resulting in a tensor containing embeddings for all input sequences.</li> </ul> <p>Please note that executing this step may take a substantial amount of time due to its computational complexity.</p> <pre><code>import torch\n\n# Function to generate embeddings for each batch\ndef batch_embed(inputs):\n    embeddings = []\n    for input in inputs:\n        with torch.no_grad():\n            outputs = model(**input)\n            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n            embeddings.append(batch_embeddings)\n    return torch.cat(embeddings)\n\n# Generate embeddings\nembeddings = batch_embed(batched_inputs)\nimport csv\n\n# Define the file path to save the embeddings\noutput_file = \"embeddings_roberta.csv\"\n\n# Convert embeddings tensor to a numpy array\nembeddings_array = embeddings.numpy()\n\n# Write the embeddings to a CSV file\nwith open(output_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    for embedding_row in embeddings_array:\n        writer.writerow(embedding_row\n</code></pre>"},{"location":"RoBERTa_Walkthrough/#topic-modeling","title":"Topic Modeling","text":"<pre><code>import pandas as pd\nimport numpy as np\n\n# Load the embeddings from the CSV file\ndf = pd.read_csv(\"embeddings_roberta.csv\", header=None)\nembeddings = df.values\n\n# Create a BERTopic instance without specifying an embedding model\ntopic_model = BERTopic()\n\n# Fit the topic model and get topics and probabilities\ntopics, probabilities = topic_model.fit_transform(abstracts, embeddings)\n</code></pre>"},{"location":"RoBERTa_Walkthrough/#visualizing-analyzing-and-comparing-results","title":"Visualizing, Analyzing and Comparing Results","text":"<p>Similar to what we have produced above, we are first looking at the Intertopic Distance Map. <pre><code>topic_model.visualize_topics()\ntopic_model.visualize_hierarchy()\ntopic_model.get_topic_info()\ntopic_model.get_topics()\n</code></pre></p>"},{"location":"RoBERTa_Walkthrough/#improve-and-fine-tune","title":"Improve and Fine-Tune","text":"<p>Clearly, we can see the performance of this model is not ideal, as there are many stop words that influence the quality of our output. Stop words such as \"we,\" \"the,\" \"of,\" and \"and\" are common and do not carry significant meaning, which can dilute the meaningful patterns in our data and negatively impact the performance of our NLP model. To improve the performance, we can pre-process the textual dataset as follows:  </p> <pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nimport re\n\n# Load the dataset again\ndf = pd.read_csv('Web_of_Science_Query May 07 2024_1-5000.csv')\nabstracts = df['Abstract'].dropna().tolist()\n\n# Define a pre-processing function\ndef preprocess(text):\n    text = text.lower()  # Convert to lowercase\n    text = re.sub(r'\\d+', '', text)  # Remove numbers\n    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    words = text.split()\n    words = [word for word in words if word not in ENGLISH_STOP_WORDS]  # Remove stop words\n    return ' '.join(words)\n\n# Preprocess the abstracts\nabstracts = [preprocess(abstract) for abstract in abstracts]\n</code></pre> <p>Then we repeat the analysis again:</p> <pre><code>from transformers import RobertaTokenizer, RobertaModel\nimport torch\n\n# Again, load RoBERTa model and tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nmodel = RobertaModel.from_pretrained('roberta-base')\n\n# Function to tokenize text in batches\ndef batch_tokenize(texts, batch_size=32):\n    all_inputs = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i + batch_size]\n        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512)\n        all_inputs.append(inputs)\n    return all_inputs\n\n# Function to generate embeddings for each batch\ndef batch_embed(inputs):\n    embeddings = []\n    for input in inputs:\n        with torch.no_grad():\n            outputs = model(**input)\n            batch_embeddings = outputs.last_hidden_state.mean(dim=1)\n            embeddings.append(batch_embeddings)\n    return torch.cat(embeddings)\n\n# Generate embeddings\nbatched_inputs = batch_tokenize(abstracts)\nembeddings = batch_embed(batched_inputs)\n\n# Save the updated embeddings\noutput_file = \"embeddings_roberta_updated.csv\"\n\n# Convert this embeddings tensor to a numpy array\nembeddings_array = embeddings.numpy()\n\n# Write the new embeddings to a CSV file\nwith open(output_file, 'w', newline='') as csvfile:\n    writer = csv.writer(csvfile)\n    for embedding_row in embeddings_array:\n        writer.writerow(embedding_row)\ndf = pd.read_csv(\"embeddings_roberta_updated.csv\", header=None)\nembeddings = df.values\n\n# Create a BERTopic instance without specifying an embedding model\ntopic_model = BERTopic()\n\n# Fit the topic model and get topics and probabilities\ntopics, probabilities = topic_model.fit_transform(abstracts, embeddings)\ntopic_model.visualize_topics() # Visualize the topics\ntopic_info = topic_model.get_topic_info()\nprint(\"Optimized Topic Information:\")\nprint(topic_info.head(10))  # Print the top 10 topics\ntopic_model.visualize_barchart(top_n_topics=15)\ntopic_model.visualize_hierarchy()\ntopic_model.visualize_heatmap()\n</code></pre>"},{"location":"data_sources/","title":"Data Source","text":""},{"location":"data_sources/#web-of-science","title":"Web of Science","text":"<p>Web of Science is a widely used research platform and citation database that provides access to a vast collection of scholarly literature and scientific information across various disciplines. One of the key features of Web of Science is its citation indexing, which allows users to track and analyze citation metrics, such as citation counts, h-index, and citation networks. In addition to citation data, Web of Science offers powerful search and discovery tools, advanced filtering options, and analytical capabilities to facilitate literature review, bibliometric analysis, and knowledge exploration. It provides access to high-quality, peer-reviewed content from reputable publishers and scholarly organizations, making it a valuable resource for academic research, scientific publishing, and decision-making in various fields.</p> <p>In this demonstration, I will elucidate a step-by-step process for gathering and refining original data for subsequent analysis.</p> <p>Commencing with a predefined query, accessible via this link: https://www.webofscience.com/wos/woscc/summary/a09550d4-2eb9-4b8c-9f9e-711acfe751a0-e65d872d/relevance/1, I have delineated the search parameters to encompass the topics of \"environment\" and \"nature,\" incorporating keywords like \"sustainability\". Furthermore, I have delineated the search category as \"Environmental Sciences,\" specified English as the language, and selected \"Article\" as the document type. Notably, the search results are filtered to exclusively include \"Open Access\" documents.</p> <p>Web of Science permits the export of up to 1000 records per search iteration. As such, I've compiled the top 5000 records into five Excel files, each sharing a uniform structure: \"Web_of_Science_Search_1-1000 results.xls,\" \"Web_of_Science_Search_1001-2000 results.xls,\" and so forth. Now, the objective is to amalgamate these files into a singular CSV file named \"Web_of_Science_Query May 07 2024_1-5000.csv,\" retaining select columns, namely \"Publication Type,\" \"Authors,\" \"Article Title,\" \"Source Title,\" \"Abstract,\" \"Publication Year,\" and \"DOI.\"</p> <p>NB</p> <p>To use Web of Science and export datasets, you need to register.</p> <p>If you wish to use a pre-processed dataset in this demonstration, all datasets must be uploaded to the Colab Files in advance; otherwise, they will not be automatically loaded. All files are available on the GitHub repository.</p>"},{"location":"key_concepts/","title":"Key Concepts","text":""},{"location":"key_concepts/#embeddings","title":"Embeddings","text":"<p>An embedding is a representation of words, phrases, sentences, or documents as vectors in a high-dimensional space. In NLP, words are typically represented as dense numerical vectors, where each dimension of the vector represents a different aspect or feature of the word.</p> <p>Word embeddings are learned from large corpora of text using techniques like Word2Vec, GloVe, or BERT. These techniques capture semantic relationships between words, such as similarity and context, by placing similar words closer together in the embedding space.</p> <p>Embeddings are crucial in NLP tasks because they allow machine learning models to process and understand textual data more effectively. They enable the models to leverage the semantic information encoded in the vectors to make accurate predictions or perform various tasks, such as text classification, named entity recognition, machine translation, and sentiment analysis.</p> <p>More specifically, word embeddings, such as Word2Vec or GloVe, each token is represented as a dense vector in a high-dimensional space. The values in this vector represent the token's semantic meaning or context. The weights of these vectors are learned during the training process, where the model adjusts them to optimize performance on a specific task, such as predicting nearby words in a sentence.</p>"},{"location":"key_concepts/#dimension-reduction","title":"Dimension Reduction","text":"<p>Dimension reduction is a technique used in machine learning and statistics to reduce the number of variables or features under consideration. The goal of dimension reduction is to simplify the dataset while preserving important information.</p> <p>There are several reasons why dimension reduction might be applied:</p> <p>Curse of Dimensionality: With an increase in the number of features, the volume of data space grows exponentially, leading to sparsity and computational inefficiency.</p> <p>Visualization: High-dimensional data is difficult to visualize directly. Dimension reduction techniques help in visualizing data in lower-dimensional space, such as 2D or 3D, while preserving its structure and relationships.</p> <p>Noise Reduction: Dimension reduction can help in removing noise and irrelevant features, which can improve the performance of machine learning models by focusing on the most important aspects of the data.</p> <p>BERTopic uses UMAP (Uniform Manifold Approximation and Projection) for dimensionality reduction.</p>"},{"location":"key_concepts/#umap","title":"UMAP","text":"<p>UMAP is a dimension reduction technique that is particularly effective for preserving local and global structure in high-dimensional data. It works by modeling the manifold of the data points and finding a low-dimensional representation that preserves the local structure of the data.</p> <p>In the context of BERTopic, UMAP is applied to the embeddings generated by BERT for the input documents. These embeddings capture semantic information about the documents, and UMAP reduces their dimensionality while retaining the relevant structure. This reduced-dimensional representation of the document embeddings is then used for clustering and topic modeling within BERTopic.</p>"},{"location":"key_concepts/#cluster","title":"Cluster","text":"<p>In general, a \"cluster\" refers to a group of data points that are similar or closely related to each other according to a specific criterion or measure. Clustering algorithms aim to partition a dataset into clusters such that data points within the same cluster are more similar to each other than they are to data points in other clusters. The goal is to identify meaningful patterns or structures in the data, which can aid in data analysis, pattern recognition, and decision-making tasks.</p>"},{"location":"key_concepts/#hdbscan","title":"HDBSCAN","text":"<p>HDBSCAN, or Hierarchical Density-Based Spatial Clustering of Applications with Noise, is a clustering algorithm used in machine learning and data analysis. It builds upon the concepts of density-based clustering to automatically determine the number of clusters in a dataset while robustly handling noise and outliers. HDBSCAN forms clusters by identifying regions of high density in the data space, allowing for flexible cluster shapes and varying cluster sizes.</p>"},{"location":"key_concepts/#tokenization","title":"Tokenization","text":"<p>Tokenization is a process used in NLP to break down text into smaller units called tokens. These tokens can be individual words, phrases, or other meaningful elements of the text, depending on the specific tokenization rules applied.</p> <p>For example, in English text, tokenization typically involves splitting the text into words based on spaces and punctuation. However, more advanced tokenization techniques may also handle special cases like contractions, hyphenated words, or even subword units for languages with complex morphology.</p> <p>Tokenization is a fundamental preprocessing step in many NLP tasks, such as text classification, named entity recognition, and machine translation, as it helps to standardize and structure the text data for further analysis by NLP models.</p>"},{"location":"key_concepts/#countvectorizer","title":"CountVectorizer","text":"<p>CountVectorizer is a text vectorization technique commonly used in NLP and machine learning tasks. It converts a collection of text documents into a matrix of token counts.</p> <p>Here is how it works:</p> <ol> <li> <p>Tokenization: CountVectorizer first tokenizes the input text documents. It typically breaks down the text into individual words or terms, called tokens.</p> </li> <li> <p>Vocabulary Building: Next, it builds a vocabulary of unique tokens from the entire corpus of text documents. Each unique token becomes a feature in the vectorized representation.</p> </li> <li> <p>Count Encoding: For each document, CountVectorizer counts the occurrences of each token in the document and encodes this count into the corresponding feature in the vector representation. Each document is thus represented by a vector where each element corresponds to the count of a specific token in that document.</p> </li> </ol> <p>The resulting matrix, often referred to as a \"document-term matrix,\" represents the frequency of each term (token) in each document. This matrix can then be used as input to machine learning algorithms for tasks such as text classification, clustering, or information retrieval.</p> <p>CountVectorizer is a simple and efficient way to convert text data into a format that machine learning models can understand, but it does not capture the semantic meaning of words or their context.</p>"},{"location":"key_concepts/#tf-idf","title":"TF-IDF","text":"<p>Term Frequency (TF): This component measures how often a term appears in a document. It is calculated as the number of times a term occurs in a document divided by the total number of terms in the document.</p> <p>Inverse Document Frequency (IDF): This component measures the rarity of a term across the entire corpus of documents. It is calculated as the logarithm of the total number of documents divided by the number of documents containing the term.</p> <p>In TF-IDF, each token's weight is calculated based on its frequency in the document (TF) and its rarity across the entire corpus of documents (IDF). Tokens that appear frequently in a document but rarely across the corpus are considered more important and receive higher weights, while common tokens receive lower weights.</p>"},{"location":"key_concepts/#c-tf-idf","title":"c-TF-IDF","text":"<p>Class Frequency (CF): In C-TF-IDF, an additional component called Class Frequency is introduced. This component measures how often a term appears in documents belonging to a specific class or category. It is calculated as the number of documents containing the term within the class divided by the total number of documents in that class.</p> <p>The C-TF-IDF weight for each term is calculated by multiplying its TF, IDF, and CF values. This results in a weighting scheme that not only considers the importance of a term within a document and across the entire corpus but also takes into account its relevance to a specific class or category.</p> <p>C-TF-IDF is particularly useful in text classification tasks where documents belong to predefined classes or categories. By incorporating class-specific information into the weighting scheme, it helps improve the discriminative power of terms and enhances the performance of text classifiers.</p>"},{"location":"key_concepts/#weight-tokens","title":"Weight tokens","text":"<p>\"Weighting tokens\" typically refers to assigning weights or importance values to tokens (words or terms) in a text document. This process is very common, particularly in vectorization techniques such as TF-IDF (Term Frequency-Inverse Document Frequency) and word embeddings.</p>"},{"location":"key_concepts/#sentence-transformers","title":"Sentence-transformers","text":"<p>Sentence-transformers is a Python library for generating sentence embeddings using pre-trained transformer-based models. These models are trained to convert variable-length texts, such as sentences or paragraphs, into fixed-dimensional vectors, known as embeddings, while capturing their semantic meaning:</p> <ol> <li> <p>Pre-trained Transformer Models: Sentence-transformers leverage pre-trained transformer-based models, such as the models we are introducing in this tutorial - BERT or RoBERTa. They both have been pre-trained on large text corpora using unsupervised learning objectives.</p> </li> <li> <p>Fine-tuning or Transfer Learning: In addition to the pre-trained transformer model, sentence-transformers often employ transfer learning or fine-tuning techniques. This involves further training the model on a downstream task, such as sentence similarity, paraphrase identification, or text classification, using labeled data. Fine-tuning allows the model to adapt to specific tasks or domains and improve its performance.</p> </li> <li> <p>Embedding Generation: Once the model is trained or fine-tuned, it can generate embeddings for input sentences or text passages. These embeddings represent the semantic meaning of the input text in a fixed-dimensional vector space. Similar sentences are expected to have similar embeddings, allowing for various downstream NLP tasks, such as semantic search, text clustering, or document classification.</p> </li> </ol> <p>Sentence-transformers offer a simple and efficient way to generate high-quality sentence embeddings, which can be used in a wide range of natural language processing applications. The library provides pre-trained models and interfaces for fine-tuning, inference, and evaluation, making it easy to integrate into NLP pipelines and projects.</p>"}]}